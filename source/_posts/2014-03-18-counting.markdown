---
layout: post
title: "count() ain't nothing but a number"
date: 2014-03-18 22:31
comments: false
categories: 
---
Compter des données ou en estimer la cardinalité est une opération dont les applications sont très variées mais surtout tellement utiles que tout le monde ou presque a besoin de le faire à un moment ou à un autre. Au fil de mes expériences, j'ai pu observer de nombreux moyens de le faire en fonction des contextes et de la grande variété des outils utilisés. Pas tout le monde n'a l'ambition de se revendiquer "Data scientist", mais j'ai vu des gens faire preuve de créativité avec les moyens du bord et surtout leur propre niveau de connaissance. Bien sûr le volume de données traité reste un enjeu majeur de la question et le niveau d'exigence requis en dépend la plupart du temps. Les contraintes liés à la performance des différents systèmes ont historiquement souvent posés des problèmes à la patience des utilisateurs lors de la consultation de rapport d'analyses. Il est courant d'avoir recours à des [pré-calcul](http://en.wikipedia.org/wiki/Aggregate_%28data_warehouse%29) pour résoudre ce problème mais cela mériterait un autre article pour être développé. Cet article un peu bizarre, au lieu de se concentrer sur une technologie ou un contexte particulier, se veut être une sorte de réflexion générale de mon expérience et surtout un tour d'horizon.

#SQL
Lorsque les données sont bien rangées dans un SGBDR (en état de marche), les possibilités et surtout l'accessibilité du langage SQL n'échappent à personne. Count, clause Where, Group By, sont simple à mettre en œuvre grâce aux commodités du langage SQL. Quand les choses se compliquent, la palette d'outils des principaux systèmes répond aux cas d'utilisation les plus tordus (DISTINCT, EXISTS, HAVING, etc).

![mysql]({{ root_url }}/images/mysql-count.png "mysql count() group by")

Avec beaucoup de données, viennent les problèmes. Un des outils intéressant concernant le comptage et la cardinalité et parfois méconnu est l'index [bitmap](http://en.wikipedia.org/wiki/Bitmap_index). Si vous ne le connaissez pas, je vous encourage à étudier son principe de fonctionnement. Ci dessous un [plan d'execution](http://en.wikipedia.org/wiki/Query_plan) mettant à contribution un index bitmap sous Oracle 11g:

![bitmap]({{ root_url }}/images/bitmap.png "bitmap plan")

Dans la famille des systèmes relationnels, une autre approche prometteuse souhaite répondre à la problématique du volume de données: les [ bases vectorielles](http://en.wikipedia.org/wiki/Vectorwise). Basé sur une orientation [colonne](http://en.wikipedia.org/wiki/Column-oriented_DBMS) tout en gardant la commodité de l'interrogation d'un modèle relationnel, cette base promet des performances incroyables. Gardons à l'esprit que le gain se fait bien sûr au détriment de quelques sacrifices comme les contraintes ou le principe de transaction. Privé de polyvalence, elles sont exclues de nombreux domaines d'applications opérationnels.

#Non-hacker
Dans ce monde (et j'y inclus des professionnels de l'informatique), les tâches courantes de comptage sont souvent laborieuses quand elles ne sont carrément pas ignorées. Selon mon expérience, le problème est souvent résolu tant bien que mal avec un tableur directement sur une station de travail. Personnellement, j'appelle ça du bricolage, mais j'avoue être parfois impressionné par la créativité mise en œuvre. Dans le meilleur des cas on a recours à un [tableau croisé dynamique](http://en.wikipedia.org/wiki/Pivot_table). La mise en forme des données nécessite souvent un travail d’orfèvre. Heureusement, il existe au moins un [frein](http://stackoverflow.com/questions/526921/why-is-there-still-a-row-limit-in-microsoft-excel) à l'utilisation parfois désastreuse de ce genre d'utilisations quand les besoins deviennent récurrents.

#Hacker
Comme souvent, UNIX a une réponse simple, efficace et robuste à la question. Pourquoi perdre du temps avec de nouveaux outils quand ceux qu'on a déjà conviennent ? La puissance du modèle UNIX fournit beaucoup de souplesse et permet d'effectuer de nombreuses opérations en cumulant successivement plusieurs fonctionnalités grâce à l'utilisation du [pipe](http://doc.cat-v.org/unix/pipes/). Pour ne citer que quelques-uns de ces indispensables outils:

- sort
- uniq
- grep
- wc
- join
- awk
- cut

En plus d'être bien documentés, open source et faciles à utiliser, ces outils sont généralement [très](http://lists.freebsd.org/pipermail/freebsd-current/2010-August/019310.html) [efficients](http://en.wikipedia.org/wiki/Merge_sort).

Une fois qu'on s'y est habitué, il est difficile de s'en passer. J'ai découvert que même lorsqu'on ne disposait que d'un environnement Windows, il est possible de profiter de quelques-uns de ces outils grâce à certaines solutions comme [MKStoolkit](http://en.wikipedia.org/wiki/MKS_Toolkit) ou [cygwin](http://en.wikipedia.org/wiki/Cygwin). Précisons que c'est une utilisation dégradée par rapport aux outils originaux, car selon mon expérience la qualité du portage et de l'implémentation laissent largement à désirer.

![mkstoolkit]({{ root_url }}/images/mkstoolkit.png "nutcracker")

Dans le monde de l'entreprise, les progiciels d'[ETL](http://en.wikipedia.org/wiki/Extract,_transform,_load) sont démocratisés et peuvent réaliser des opérations de comptage. Ci-dessous, vous trouverez un exemple d'implémentation d'une opération sur des fichiers. On peut constater que la logique d'élaboration d'un flux de données au travers de l'interface graphique dite "conviviale", s'inspire largement du modèle Unix cité précédemment. Les données sont traitées successivement dans des modules spécifiques, et on peut facilement assimiler les liens entre ces fonctions au "pipe".

![datastage]({{ root_url }}/images/datastage.png "datastage")

Pour des besoins bien spécifiques, il existe des librairies dédiées dans la plupart des langages de programmation courants. On peut notamment citer, [panda](http://pandas.pydata.org/), un framework de traitement en mémoire qui fait beaucoup parler de lui parcequ'il est facile à mettre en œuvre même sur une station de travail et surtout qu'il est très performant.

Parmi les bases de données non relationnels qui ont tendances à se démocratiser, comme les bases orientées documents, la manipulation des données n'est généralement pas au cœur des préoccupations et les outils disponibles ont souvent sommaires pour dire le moins. Citons le framework d'aggregation de MongoDB apparu seulement dans la version 2.4 qui apporte une bouffée d’oxygène pour quiconque souhaite manipuler une collection malgré des performances souvent décevantes. L'optimisation des performances des requêtes d'analyses sur ce genre de système est véritablement un art. La modélisation des données en est d'ailleurs un enjeu souvent plus subtile qu'on ne le pense. L'utilisation des indexes malgré une amélioration des outils comme ".explain()" ou ".hint()", reste difficile et selon mon expérience quelquefois obscure malheureusement. Pour illustrer ces propos voici un exemple basique de comptage sur une collection comportant un index. Le plan d’exécution d'une interrogation simple sur la collection montre qu'on ne sollicite que l'index (index_only: true):


Ce type d'accès pourrait en théorie profiter au comptage de cardinalité, malheureusement en consultant le plan d’exécution de la fonction ".aggregate()" (disponible depuis la version 2.6 du mongoshell) que l'on souhaite utiliser et on peut constater que l’accès se fait par un curseur "basique" qui semble ignorer l'index :(


#BIG DATA
Quand les volumes dépassent ce que savent faire les solutions décrient plus haut, on entre dans la haute voltige (et les buzzwords) surtout quand en plus on souhaite se rapprocher du traitement en [temps réél](https://developers.soundcloud.com/blog/real-time-counts-with-stitch).

A mon avis, il n'y a pas autant de domaines concernés qu'on aimerait nous le faire croire. Toutefois si l'envie de faire des comptages massivement parallèles sur des fichiers de logs en mode batch vous prend, vous serez sûrement amené à utiliser l'usine à gaz Hadoop et son HDFS. Heureusement pour vous, "regrouper" des données sur une clef est une fonction qui est au cœur du paradigme map/reduce !

En effet le comptage de cardinalité est plutôt simple à implémenter puisqu'on se sert principalement du comportement déjà existant du framework. Basiquement: toutes les valeurs de clefs sont regroupées et rassemblées dans le "reducer". Il suffit de sélectionner les champs sur lesquels on souhaite faire un regroupement dans le "mapper", le regroupement en lui même est entièrement pris en charge par le framework !

![map_reduce]({{ root_url }}/images/mapreduce_sum.png "mapreduce1")

Voici les quelques lignes de java qui suffisent à implémenter un comptage de cardinalité dans le "reducer":

``` java
public static class CountReducer extends Reducer<Text, MinMaxCountTuple, Text, MinMaxCountTuple> {
	// Our output value Writable
	private CountTuple result = new CountTuple();

	public void reduce(Text key, Iterable<CountTuple> values,Context context) throws IOException, InterruptedException {
		// Initialize our result
		result.setCount(0);
		int sum = 0;

		// Iterate through all input values for this key
		for (CountTuple val : values) {
			// Add to our sum the count for value
			sum += val.getCount();
		}
		// Set our count to the number of input values
		result.setCount(sum);
		context.write(key, result);
	}
}
```

![map_reduce]({{ root_url }}/images/mapreduce_combine.png "mapreduce2")

Attendez, il y a aussi autre chose qui devrait vous plaire. Plutôt que de parcourir intégralement d'immenses quantités d'information au prix d'une lourde consommation de mémoire, que pensez vous d'en faire une estimation ? Basé sur les travaux du défunt chercheur français [Philippe Flajolet](http://inria-alumni.fr/index.php/fr/page/article/id/202), une famille d'algorithme proprement révolutionnaire promet de compter les éléments uniques dans une collection (Hyperloglog) au prix d'une très faible approximation (faux positifs) ou de s'assurer de la présence d'une clef (Bloomfilter) avec très peu de consommation de mémoire. Je crois qu'il est normal de penser que c'est impossible et pourtant... de nombreuses applications peuvent profiter de ces outils. Un cas célèbre d'utilisation a été documenté par "google", évidemment pionnier en la matière, qui évite de parcourir un fichier si la clef recherché n'y figure pas.

Notez bien qu'on peut désormais tester et utiliser ces outils grâces à la mise à disposition de librairies faciles à mettre en oeuvre. J'ai pour ma part fait quelques essais avec une implémentation en [java](https://github.com/addthis/stream-lib) que je ne saurai trop vous conseiller car elle est excellente.

![count]({{ root_url }}/images/count-bundesliga.png "count unixVSbloom")

Avant de finir, j'aimerais évoquer Redis, qui est sûrement un des outils les plus prometteurs dont j'ai entendu parlé. Il propose notamment l’implémentation d'outils très performants pour le comptage de la cardinalité comme les ["bitsets"](http://blog.getspool.com/2011/11/29/fast-easy-realtime-metrics-using-redis-bitmaps/). 


### Ressources

- les [données](http://www.football-data.co.uk/germanym.php) du championnat de football allemand que j'ai utilisé pour certains exemples
- un [exemple détaillé](http://web.archive.org/web/20130113080935/http://backstage.soundcloud.com/2011/07/mysql-stats-old-faithful/) de problèmes de performance avec mysql et de grosses volumétries
- un [article](http://www.gregreda.com/2013/07/15/unix-commands-for-data-science/) sur les outils d'UNIX en lignes de commande
- un rare [tutoriel](http://sametmax.com/le-pandas-cest-bon-mangez-en/) sur l'utilisation de panda en français
- [quelques](http://web.archive.org/web/20100128030005/http://kylebanker.com/blog/2009/11/mongodb-count-group) [articles](http://fiestacc.tumblr.com/post/11319522700/walkthrough-mongodb-data-modeling) fondateurs sur les subtilités d'utilisation de MongoDB
- un [article complet](http://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/) sur les différentes techniques de modélisation "NOSQL"
- un [ouvrage](http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176) de référence sur le framework Map/Reduce
- une [vidéo](https://www.youtube.com/watch?v=oIkhgagvrjI) amusante d'explications sur un des compteurs les plus célèbres de notre époque